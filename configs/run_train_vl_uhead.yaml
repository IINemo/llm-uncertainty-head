hydra:
  run:
    dir: ${output_dir}/${now:%Y-%m-%d}/${now:%H-%M-%S}

model:
  pretrained_model_name_or_path: google/gemma-3-12b-it
  device_map: auto
  trust_remote_code: true
  torch_dtype: torch.bfloat16
  is_vlm: true  # Set to true for Vision-Language Models

ue_layer:
  path: ''
  pos_weight: 3
  output_attention: true
  head_cfg:
    head_type: claim
    feature_extractor:
    - name: luh.feature_extractors.basic_hidden_states
      layer_nums: all
    - name: luh.feature_extractors.token_probabilities
      top_n: 4
    - name: luh.feature_extractors.basic_attention
      layer_nums: all
      attn_history_sz: 3
      pool: false
    uncertainty_head:
      head_dim: 768
      n_layers: 2
      n_heads: 8
      dropout: 0.1

dataset:
  path: hf:nhatkhangdtp/uncertainty-vlm-gemma
  num_instances: 0
  test_size: 0.0
  validation: test
  image_column: image  # Column name containing images

training_arguments:
  num_train_epochs: 10
  learning_rate: 0.0001
  warmup_ratio: 0.05
  weight_decay: 0.1
  gradient_accumulation_steps: 1
  per_device_train_batch_size: 4  # Smaller batch size for VLMs due to image memory
  per_device_eval_batch_size: 2 
  max_grad_norm: 1.0

do_train: true
do_eval: true
do_hyperopt: false
do_save_checkpoints: false
do_save_final_model: false
report_to: none
output_dir: ./workdir/train_vl
do_predict: false
