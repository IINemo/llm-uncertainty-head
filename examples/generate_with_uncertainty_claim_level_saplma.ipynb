{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"add your key here\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=False,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "generation_config = GenerationConfig.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_polygraph.model_adapters import WhiteboxModelBasic\n",
    "from lm_polygraph.stat_calculators.extract_claims import ClaimsExtractor\n",
    "from lm_polygraph.utils.openai_chat import OpenAIChat\n",
    "\n",
    "from luh.luh_claim_estimator import LuhClaimEstimator\n",
    "from luh.calculator_infer_luh import CalculatorInferLuh\n",
    "from luh.auto_uncertainty_head import AutoUncertaintyHead\n",
    "\n",
    "\n",
    "model_adapter = WhiteboxModelBasic(model=model, \n",
    "                                   tokenizer=tokenizer, \n",
    "                                   tokenizer_args={\"add_special_tokens\": False, \n",
    "                                                   \"return_tensors\": \"pt\", \n",
    "                                                   \"padding\": True, \"truncation\": True},\n",
    "                                   model_type=\"CausalLM\")\n",
    "model_adapter.model_path = model_name\n",
    "\n",
    "args_generate = {\"generation_config\": generation_config,\n",
    "                 \"max_new_tokens\": 50}\n",
    "uq_head = \"llm-uncertainty-head/saplma_Mistral-7B-Instruct-v0.2\"\n",
    "uncertainty_head = AutoUncertaintyHead.from_pretrained(uq_head, base_model=model)\n",
    "calc_infer_llm = CalculatorInferLuh(uncertainty_head, \n",
    "                                    tokenize=True, \n",
    "                                    args_generate=args_generate,\n",
    "                                    device=\"cuda\",\n",
    "                                    generations_cache_dir=\"\"\n",
    "                                    )\n",
    "\n",
    "openai_chat = OpenAIChat(cache_path='./workdir/cache', openai_model=\"gpt-4o\")\n",
    "calc_extract_claims = ClaimsExtractor(openai_chat=openai_chat)\n",
    "\n",
    "estimator = LuhClaimEstimator(reduce_type=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"How many fingers are on a coala's foot?\"\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Who sang a song Yesterday?\"\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Who sang a song Кукла Колдуна?\"\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Translate into French: 'I want a small cup of coffee'\"\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "\n",
    "chat_messages = [tokenizer.apply_chat_template(m, tokenize=False, add_bos_token=False) for m in messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "data_loader = DataLoader(chat_messages, batch_size=2,\n",
    "                         shuffle=False, collate_fn=lambda x: x)\n",
    "\n",
    "all_generated_tokens = []\n",
    "all_claims = []\n",
    "all_uncertainties = []\n",
    "for texts in tqdm(data_loader):\n",
    "    deps = dict()\n",
    "    print(\"Performing inference...\")\n",
    "    deps.update(calc_infer_llm(deps, texts=texts, model=model_adapter))\n",
    "    print(model_adapter.tokenizer.decode(deps['greedy_tokens'][0]))\n",
    "    print(\"Extracting claims...\")\n",
    "    deps.update(calc_extract_claims(\n",
    "        deps, texts=texts, model=model_adapter))\n",
    "    print(\"Estimating uncertainty...\")\n",
    "    uncertainty_score = estimator(deps)\n",
    "\n",
    "    all_generated_tokens += deps['greedy_tokens']\n",
    "    all_claims += deps['claims']\n",
    "    all_uncertainties += uncertainty_score\n",
    "    print(\"Results:\")\n",
    "    for doc_claims, doc_ues in zip(deps['claims'], uncertainty_score):\n",
    "        for claim, ue in zip(doc_claims, doc_ues):\n",
    "            print(ue, claim)\n",
    "        \n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_html_tokens(\n",
    "    token_ids,\n",
    "    positions_to_highlight,\n",
    "    tokenizer,\n",
    "    color=\"red\",\n",
    "    font_weight=\"bold\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert a list of token IDs into a readable string, highlight tokens at\n",
    "    the specified positions in `positions_to_highlight`, and remove the leading\n",
    "    '▁' that Mistral/Llama tokenizers use for word boundaries.\n",
    "    \n",
    "    Args:\n",
    "        token_ids (List[int]): The sequence of token IDs.\n",
    "        tokenizer: A Hugging Face tokenizer (e.g., for mistralai/Mistral-7B-Instruct-v0.2).\n",
    "        positions_to_highlight (Set[int] or List[int]): 0-based indices of tokens to highlight.\n",
    "        color (str): CSS color for the highlighted text (default \"red\").\n",
    "        font_weight (str): CSS font weight (default \"bold\").\n",
    "    \n",
    "    Returns:\n",
    "        str: An HTML string with some tokens highlighted.\n",
    "    \"\"\"\n",
    "    # Convert the IDs to subword tokens (may contain leading \"▁\")\n",
    "    raw_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    \n",
    "    # Ensure positions_to_highlight is a set for quick membership check\n",
    "    if not isinstance(positions_to_highlight, set):\n",
    "        positions_to_highlight = set(positions_to_highlight)\n",
    "    \n",
    "    final_pieces = []\n",
    "    \n",
    "    for idx, token in enumerate(raw_tokens):\n",
    "        # If the token starts with \"▁\", replace that with a literal space\n",
    "        if token.startswith(\"▁\"):\n",
    "            display_str = \" \" + token[1:]\n",
    "        else:\n",
    "            display_str = token\n",
    "        \n",
    "        # If this position is in positions_to_highlight, wrap in <span>\n",
    "        if idx in positions_to_highlight:\n",
    "            display_str = (\n",
    "                f\"<span style='color:{color}; font-weight:{font_weight};'>\"\n",
    "                f\"{display_str}\"\n",
    "                \"</span>\"\n",
    "            )\n",
    "        \n",
    "        final_pieces.append(display_str)\n",
    "    \n",
    "    # Join everything without extra spaces\n",
    "    return \"\".join(final_pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "def highlight_uncertain_claims(uncertainties, generated_tokens, claims):\n",
    "    threshold = 0.5\n",
    "    tokens_to_highlight = set()\n",
    "\n",
    "    for ue_score, claim in zip(uncertainties, claims):\n",
    "        if ue_score > threshold:\n",
    "            tokens_to_highlight.update(claim.aligned_token_ids)\n",
    "    \n",
    "    display(HTML(highlight_html_tokens(generated_tokens, tokens_to_highlight, model_adapter.tokenizer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2\n",
    "highlight_uncertain_claims(all_uncertainties[idx], all_generated_tokens[idx], all_claims[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
