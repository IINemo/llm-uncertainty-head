{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<PUT YOUR KEY HERE>\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-29 08:30:03,418] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/artemshelmanov/conda/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/artemshelmanov/conda/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d7ca2ddcd6494eb175f06a3cde6eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=False,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "generation_config = GenerationConfig.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_polygraph.model_adapters import WhiteboxModelBasic\n",
    "from lm_polygraph.stat_calculators.extract_claims import ClaimsExtractor\n",
    "from lm_polygraph.utils.openai_chat import OpenAIChat\n",
    "\n",
    "from luh.luh_claim_estimator import LuhClaimEstimator\n",
    "from luh.calculator_infer_luh import CalculatorInferLuh\n",
    "from luh.auto_uncertainty_head import AutoUncertaintyHead\n",
    "\n",
    "\n",
    "model_adapter = WhiteboxModelBasic(model=model, \n",
    "                                   tokenizer=tokenizer, \n",
    "                                   tokenizer_args={\"add_special_tokens\": False, \n",
    "                                                   \"return_tensors\": \"pt\", \n",
    "                                                   \"padding\": True, \"truncation\": True},\n",
    "                                   model_type=\"CausalLM\")\n",
    "model_adapter.model_path = model_name\n",
    "\n",
    "args_generate = {\"generation_config\": generation_config,\n",
    "                 \"max_new_tokens\": 50}\n",
    "uq_head = \"llm-uncertainty-head/saplma_Mistral-7B-Instruct-v0.2\"\n",
    "uncertainty_head = AutoUncertaintyHead.from_pretrained(uq_head, base_model=model)\n",
    "calc_infer_llm = CalculatorInferLuh(uncertainty_head, \n",
    "                                    tokenize=True, \n",
    "                                    args_generate=args_generate,\n",
    "                                    device=\"cuda\",\n",
    "                                    generations_cache_dir=\"\"\n",
    "                                    )\n",
    "\n",
    "openai_chat = OpenAIChat(cache_path='./workdir/cache', openai_model=\"gpt-4o\")\n",
    "calc_extract_claims = ClaimsExtractor(openai_chat=openai_chat)\n",
    "\n",
    "estimator = LuhClaimEstimator(reduce_type=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"How many fingers are on a coala's foot?\"\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Who sang a song Yesterday?\"\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Who sang a song Кукла Колдуна?\"\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Translate into French: 'I want a small cup of coffee'\"\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "\n",
    "chat_messages = [tokenizer.apply_chat_template(m, tokenize=False, add_bos_token=False) for m in messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing inference...\n",
      "A koala's paw has five digits, similar to a human hand. So, each koala foot has five fingers or toes.</s>\n",
      "Extracting claims...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:30<00:30, 30.54s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating uncertainty...\n",
      "Results:\n",
      "0.49609678983688354 Claim(claim_text=\"A koala's paw has five digits.\", sentence=\"A koala's paw has five digits, similar to a human hand\", aligned_token_ids=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
      "0.418239027261734 Claim(claim_text=\"A koala's paw is similar to a human hand.\", sentence=\"A koala's paw has five digits, similar to a human hand\", aligned_token_ids=[0, 1, 2, 3, 4, 5, 6, 12, 13, 14, 15, 16])\n",
      "0.6246702671051025 Claim(claim_text='Each koala foot has five fingers or toes.', sentence=' So, each koala foot has five fingers or toes', aligned_token_ids=[20, 21, 22, 23, 24, 25, 26, 27, 28])\n",
      "\n",
      "\n",
      "0.2481205314397812 Claim(claim_text='The song \"Yesterday\" was written by Paul McCartney.', sentence='The song \"Yesterday\" was written and originally performed by the British singer-songwriter Paul McCartney for the Beatles', aligned_token_ids=[0, 1, 2, 3, 4, 5, 6, 7, 11, 19, 20, 21, 22])\n",
      "0.2762705981731415 Claim(claim_text='The song \"Yesterday\" was originally performed by Paul McCartney.', sentence='The song \"Yesterday\" was written and originally performed by the British singer-songwriter Paul McCartney for the Beatles', aligned_token_ids=[0, 1, 2, 3, 4, 5, 6, 9, 10, 11, 19, 20, 21, 22])\n",
      "0.27765581011772156 Claim(claim_text='Paul McCartney is a British singer-songwriter.', sentence='The song \"Yesterday\" was written and originally performed by the British singer-songwriter Paul McCartney for the Beatles', aligned_token_ids=[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22])\n",
      "0.38904669880867004 Claim(claim_text='The song \"Yesterday\" was performed for the Beatles.', sentence='The song \"Yesterday\" was written and originally performed by the British singer-songwriter Paul McCartney for the Beatles', aligned_token_ids=[0, 1, 2, 3, 4, 5, 10, 23, 24, 25, 26])\n",
      "0.6038522720336914 Claim(claim_text='He wrote the melody for the song in 1964.', sentence=' He wrote the melody for the song in 1964, but the lyrics came to him in a dream in 1965', aligned_token_ids=[28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41])\n",
      "0.6322610378265381 Claim(claim_text='The lyrics came to him in a dream in 1965.', sentence=' He wrote the melody for the song in 1964, but the lyrics came to him in a dream in 1965', aligned_token_ids=[30, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57])\n",
      "0.5092925429344177 Claim(claim_text='The Beatles recorded the song.', sentence=' The Beatles recorded the song for their album \"Help', aligned_token_ids=[59, 60, 61, 62, 63, 64])\n",
      "0.4010780155658722 Claim(claim_text='The song was for their album \"Help\".', sentence=' The Beatles recorded the song for their album \"Help', aligned_token_ids=[59, 64, 65, 66, 67, 68, 69])\n",
      "\n",
      "\n",
      "Performing inference...\n",
      "The song \"Кукла Колдуна\" (The Doll of the Sorcerer) is a traditional Russian folk song. It is not associated with a specific artist or singer. The melody and lyrics have been passed down through generations and are commonly sung by Russian folk ensembles and choirs.</s>\n",
      "Extracting claims...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:47<00:00, 23.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating uncertainty...\n",
      "Results:\n",
      "0.5531440377235413 Claim(claim_text='The song \"Кукла Колдуна\" (The Doll of the Sorcerer) is a traditional Russian folk song.', sentence='The song \"Кукла Колдуна\" (The Doll of the Sorcerer) is a traditional Russian folk song', aligned_token_ids=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26])\n",
      "0.25468018651008606 Claim(claim_text='It is not associated with a specific artist.', sentence=' It is not associated with a specific artist or singer', aligned_token_ids=[28, 29, 30, 31, 32, 33, 34, 35])\n",
      "0.28956034779548645 Claim(claim_text='It is not associated with a specific singer.', sentence=' It is not associated with a specific artist or singer', aligned_token_ids=[28, 29, 30, 31, 32, 33, 34, 37])\n",
      "0.279626727104187 Claim(claim_text='The melody and lyrics have been passed down through generations.', sentence=' The melody and lyrics have been passed down through generations and are commonly sung by Russian folk ensembles and choirs', aligned_token_ids=[39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49])\n",
      "0.33539214730262756 Claim(claim_text='The melody and lyrics are commonly sung by Russian folk ensembles.', sentence=' The melody and lyrics have been passed down through generations and are commonly sung by Russian folk ensembles and choirs', aligned_token_ids=[39, 40, 41, 42, 43, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "0.30716973543167114 Claim(claim_text='The melody and lyrics are commonly sung by Russian choirs.', sentence=' The melody and lyrics have been passed down through generations and are commonly sung by Russian folk ensembles and choirs', aligned_token_ids=[40, 41, 42, 43, 51, 52, 53, 54, 55, 61, 62])\n",
      "\n",
      "\n",
      "0.5354580879211426 Claim(claim_text='In French, you can say \"Je veux une tasse petite de café.\"', sentence='In French, you can say \"Je veux une tasse petite de café\" which translates to \"I want a small cup of coffee\"', aligned_token_ids=[0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17])\n",
      "0.468065083026886 Claim(claim_text='\"Je veux une tasse petite de café\" translates to \"I want a small cup of coffee.\"', sentence='In French, you can say \"Je veux une tasse petite de café\" which translates to \"I want a small cup of coffee\"', aligned_token_ids=[6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29])\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "data_loader = DataLoader(chat_messages, batch_size=2,\n",
    "                         shuffle=False, collate_fn=lambda x: x)\n",
    "\n",
    "all_generated_tokens = []\n",
    "all_claims = []\n",
    "all_uncertainties = []\n",
    "for texts in tqdm(data_loader):\n",
    "    deps = dict()\n",
    "    print(\"Performing inference...\")\n",
    "    deps.update(calc_infer_llm(deps, texts=texts, model=model_adapter))\n",
    "    print(model_adapter.tokenizer.decode(deps['greedy_tokens'][0]))\n",
    "    print(\"Extracting claims...\")\n",
    "    deps.update(calc_extract_claims(\n",
    "        deps, texts=texts, model=model_adapter))\n",
    "    print(\"Estimating uncertainty...\")\n",
    "    uncertainty_score = estimator(deps)\n",
    "\n",
    "    all_generated_tokens += deps['greedy_tokens']\n",
    "    all_claims += deps['claims']\n",
    "    all_uncertainties += uncertainty_score\n",
    "    print(\"Results:\")\n",
    "    for doc_claims, doc_ues in zip(deps['claims'], uncertainty_score):\n",
    "        for claim, ue in zip(doc_claims, doc_ues):\n",
    "            print(ue, claim)\n",
    "        \n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_html_tokens(\n",
    "    token_ids,\n",
    "    positions_to_highlight,\n",
    "    tokenizer,\n",
    "    color=\"red\",\n",
    "    font_weight=\"bold\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert a list of token IDs into a readable string, highlight tokens at\n",
    "    the specified positions in `positions_to_highlight`, and remove the leading\n",
    "    '▁' that Mistral/Llama tokenizers use for word boundaries.\n",
    "    \n",
    "    Args:\n",
    "        token_ids (List[int]): The sequence of token IDs.\n",
    "        tokenizer: A Hugging Face tokenizer (e.g., for mistralai/Mistral-7B-Instruct-v0.2).\n",
    "        positions_to_highlight (Set[int] or List[int]): 0-based indices of tokens to highlight.\n",
    "        color (str): CSS color for the highlighted text (default \"red\").\n",
    "        font_weight (str): CSS font weight (default \"bold\").\n",
    "    \n",
    "    Returns:\n",
    "        str: An HTML string with some tokens highlighted.\n",
    "    \"\"\"\n",
    "    # Convert the IDs to subword tokens (may contain leading \"▁\")\n",
    "    raw_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    \n",
    "    # Ensure positions_to_highlight is a set for quick membership check\n",
    "    if not isinstance(positions_to_highlight, set):\n",
    "        positions_to_highlight = set(positions_to_highlight)\n",
    "    \n",
    "    final_pieces = []\n",
    "    \n",
    "    for idx, token in enumerate(raw_tokens):\n",
    "        # If the token starts with \"▁\", replace that with a literal space\n",
    "        if token.startswith(\"▁\"):\n",
    "            display_str = \" \" + token[1:]\n",
    "        else:\n",
    "            display_str = token\n",
    "        \n",
    "        # If this position is in positions_to_highlight, wrap in <span>\n",
    "        if idx in positions_to_highlight:\n",
    "            display_str = (\n",
    "                f\"<span style='color:{color}; font-weight:{font_weight};'>\"\n",
    "                f\"{display_str}\"\n",
    "                \"</span>\"\n",
    "            )\n",
    "        \n",
    "        final_pieces.append(display_str)\n",
    "    \n",
    "    # Join everything without extra spaces\n",
    "    return \"\".join(final_pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "def highlight_uncertain_claims(uncertainties, generated_tokens, claims):\n",
    "    threshold = 0.5\n",
    "    tokens_to_highlight = set()\n",
    "\n",
    "    for ue_score, claim in zip(uncertainties, claims):\n",
    "        if ue_score > threshold:\n",
    "            tokens_to_highlight.update(claim.aligned_token_ids)\n",
    "    \n",
    "    display(HTML(highlight_html_tokens(generated_tokens, tokens_to_highlight, model_adapter.tokenizer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style='color:red; font-weight:bold;'> The</span><span style='color:red; font-weight:bold;'> song</span><span style='color:red; font-weight:bold;'> \"</span><span style='color:red; font-weight:bold;'>К</span><span style='color:red; font-weight:bold;'>у</span><span style='color:red; font-weight:bold;'>кла</span><span style='color:red; font-weight:bold;'> Ко</span><span style='color:red; font-weight:bold;'>л</span><span style='color:red; font-weight:bold;'>ду</span><span style='color:red; font-weight:bold;'>на</span><span style='color:red; font-weight:bold;'>\"</span><span style='color:red; font-weight:bold;'> (</span><span style='color:red; font-weight:bold;'>The</span><span style='color:red; font-weight:bold;'> D</span><span style='color:red; font-weight:bold;'>oll</span><span style='color:red; font-weight:bold;'> of</span><span style='color:red; font-weight:bold;'> the</span><span style='color:red; font-weight:bold;'> Sor</span><span style='color:red; font-weight:bold;'>cer</span><span style='color:red; font-weight:bold;'>er</span><span style='color:red; font-weight:bold;'>)</span><span style='color:red; font-weight:bold;'> is</span><span style='color:red; font-weight:bold;'> a</span><span style='color:red; font-weight:bold;'> traditional</span><span style='color:red; font-weight:bold;'> Russian</span><span style='color:red; font-weight:bold;'> folk</span><span style='color:red; font-weight:bold;'> song</span>. It is not associated with a specific artist or singer. The melody and lyrics have been passed down through generations and are commonly sung by Russian folk ensembles and choirs.</s>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 2\n",
    "highlight_uncertain_claims(all_uncertainties[idx], all_generated_tokens[idx], all_claims[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
