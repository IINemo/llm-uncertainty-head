{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51253e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"Your OpenAI API key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "576cbbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/artemshelmanov/conda/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from luh import AutoUncertaintyHead\n",
    "\n",
    "from lm_polygraph import CausalLMWithUncertainty\n",
    "from lm_polygraph.stat_calculators import ClaimsExtractor\n",
    "from lm_polygraph.utils.openai_chat import OpenAIChat\n",
    "\n",
    "from luh.calculator_infer_luh import CalculatorInferLuh\n",
    "from luh.calculator_apply_uq_head import CalculatorApplyUQHead\n",
    "from luh.luh_claim_estimator_dummy import LuhClaimEstimatorDummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e00cc0fc-cd7c-49dc-a38b-52880686091d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['output_attentions']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:09<00:00,  3.19s/it]\n"
     ]
    }
   ],
   "source": [
    "# load model and uhead\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "uhead_name = \"llm-uncertainty-head/uhead_claim_exp5_Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, device_map=\"cuda\", output_attentions=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "uhead = AutoUncertaintyHead.from_pretrained(\n",
    "    uhead_name, base_model=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63f267be-faf2-4615-906e-223e13287950",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "args_generate = {\"generation_config\": generation_config}\n",
    "calc_infer_llm = CalculatorInferLuh(\n",
    "    uhead,\n",
    "    tokenize=True,\n",
    "    args_generate=args_generate,\n",
    "    device=\"cuda\",\n",
    "    generations_cache_dir=\"\",\n",
    "    predict_token_uncertainties=False,\n",
    ")\n",
    "\n",
    "openai_chat = OpenAIChat()\n",
    "calc_steps_extractor = ClaimsExtractor(openai_chat)\n",
    "calc_apply_uhead = CalculatorApplyUQHead(uhead)\n",
    "\n",
    "estimator = LuhClaimEstimatorDummy()\n",
    "llm_adapter = CausalLMWithUncertainty(\n",
    "    llm,\n",
    "    tokenizer=tokenizer,\n",
    "    stat_calculators=[calc_infer_llm, calc_steps_extractor, calc_apply_uhead],\n",
    "    estimator=estimator,\n",
    "    keep_deps=[\"claims\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a179b2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['sequences', 'scores', 'attentions', 'hidden_states', 'past_key_values', 'full_attention_mask', 'context_lengths'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/artemshelmanov/conda/lib/python3.13/site-packages/torch/nn/modules/transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.48153509069806605,\n",
       "  0.70083100335193,\n",
       "  0.37825823693552746,\n",
       "  0.2932147494381937,\n",
       "  0.926262639949862,\n",
       "  0.7213100780196579]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare text ...\n",
    "\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"In which year did the programming language Mercury first appear?\"\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "\n",
    "chat_messages = [tokenizer.apply_chat_template(m, tokenize=False, add_bos_token=False) for m in messages]\n",
    "inputs = tokenizer(chat_messages, return_tensors=\"pt\", padding=True, truncation=True, add_special_tokens=False).to(\"cuda\")\n",
    "\n",
    "output = llm_adapter.generate(inputs[\"input_ids\"], max_new_tokens=100)\n",
    "output[\"uncertainty_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2dc2b4c-23df-49ee-8e58-ab65f2549bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response and uncertainty scores:\n",
      "Response: ['Mercury is a logic programming language that was developed between 1986 and 1990 at the University of Edinburgh in Scotland. The first official release of Mercury was in 1993. So, the first appearance of Mercury was in 1993.</s>']\n",
      "UE Scores: [0.48153509069806605, 0.70083100335193, 0.37825823693552746, 0.2932147494381937, 0.926262639949862, 0.7213100780196579]\n"
     ]
    }
   ],
   "source": [
    "print(\"Model response and uncertainty scores:\")\n",
    "print(f'Response: {tokenizer.batch_decode(output[\"sequences\"][:,len(inputs[\"input_ids\"][0]):])}')\n",
    "print(f'UE Scores: {output[\"uncertainty_score\"][0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ebd3692-c8c7-42d4-8c23-76e760994176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_html_tokens(\n",
    "    token_ids,\n",
    "    positions_to_highlight,\n",
    "    tokenizer,\n",
    "    color=\"red\",\n",
    "    font_weight=\"bold\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert a list of token IDs into a readable string, highlight tokens at\n",
    "    the specified positions in `positions_to_highlight`, and remove the leading\n",
    "    '▁' that Mistral/Llama tokenizers use for word boundaries.\n",
    "    \n",
    "    Args:\n",
    "        token_ids (List[int]): The sequence of token IDs.\n",
    "        tokenizer: A Hugging Face tokenizer (e.g., for mistralai/Mistral-7B-Instruct-v0.2).\n",
    "        positions_to_highlight (Set[int] or List[int]): 0-based indices of tokens to highlight.\n",
    "        color (str): CSS color for the highlighted text (default \"red\").\n",
    "        font_weight (str): CSS font weight (default \"bold\").\n",
    "    \n",
    "    Returns:\n",
    "        str: An HTML string with some tokens highlighted.\n",
    "    \"\"\"\n",
    "    # Convert the IDs to subword tokens (may contain leading \"▁\")\n",
    "    raw_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    \n",
    "    # Ensure positions_to_highlight is a set for quick membership check\n",
    "    if not isinstance(positions_to_highlight, set):\n",
    "        positions_to_highlight = set(positions_to_highlight)\n",
    "    \n",
    "    final_pieces = []\n",
    "    \n",
    "    for idx, token in enumerate(raw_tokens):\n",
    "        # If the token starts with \"▁\", replace that with a literal space\n",
    "        if token.startswith(\"▁\"):\n",
    "            display_str = \" \" + token[1:]\n",
    "        else:\n",
    "            display_str = token\n",
    "        \n",
    "        # If this position is in positions_to_highlight, wrap in <span>\n",
    "        if idx in positions_to_highlight:\n",
    "            display_str = (\n",
    "                f\"<span style='color:{color}; font-weight:{font_weight};'>\"\n",
    "                f\"{display_str}\"\n",
    "                \"</span>\"\n",
    "            )\n",
    "        \n",
    "        final_pieces.append(display_str)\n",
    "    \n",
    "    # Join everything without extra spaces\n",
    "    return \"\".join(final_pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef15b565-9e2d-434e-b2dc-983a654f016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "def highlight_uncertain_claims(uncertainties, generated_tokens, claims):\n",
    "    threshold = 0.5\n",
    "    tokens_to_highlight = set()\n",
    "\n",
    "    for ue_score, claim in zip(uncertainties, claims):\n",
    "        if ue_score > threshold:\n",
    "            tokens_to_highlight.update(claim)\n",
    "    \n",
    "    display(HTML(highlight_html_tokens(generated_tokens, tokens_to_highlight, tokenizer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f1a2774-fae3-4474-893a-1cddaae11cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style='color:red; font-weight:bold;'> Mer</span><span style='color:red; font-weight:bold;'>cury</span> is a logic programming language that<span style='color:red; font-weight:bold;'> was</span><span style='color:red; font-weight:bold;'> developed</span><span style='color:red; font-weight:bold;'> between</span> <span style='color:red; font-weight:bold;'>1</span><span style='color:red; font-weight:bold;'>9</span><span style='color:red; font-weight:bold;'>8</span><span style='color:red; font-weight:bold;'>6</span><span style='color:red; font-weight:bold;'> and</span> <span style='color:red; font-weight:bold;'>1</span><span style='color:red; font-weight:bold;'>9</span><span style='color:red; font-weight:bold;'>9</span><span style='color:red; font-weight:bold;'>0</span> at the University of Edinburgh in Scotland.<span style='color:red; font-weight:bold;'> The</span><span style='color:red; font-weight:bold;'> first</span><span style='color:red; font-weight:bold;'> official</span><span style='color:red; font-weight:bold;'> release</span><span style='color:red; font-weight:bold;'> of</span><span style='color:red; font-weight:bold;'> Mer</span><span style='color:red; font-weight:bold;'>cury</span><span style='color:red; font-weight:bold;'> was</span><span style='color:red; font-weight:bold;'> in</span> <span style='color:red; font-weight:bold;'>1</span><span style='color:red; font-weight:bold;'>9</span><span style='color:red; font-weight:bold;'>9</span><span style='color:red; font-weight:bold;'>3</span>.<span style='color:red; font-weight:bold;'> So</span>,<span style='color:red; font-weight:bold;'> the</span><span style='color:red; font-weight:bold;'> first</span><span style='color:red; font-weight:bold;'> appearance</span><span style='color:red; font-weight:bold;'> of</span><span style='color:red; font-weight:bold;'> Mer</span><span style='color:red; font-weight:bold;'>cury</span><span style='color:red; font-weight:bold;'> was</span><span style='color:red; font-weight:bold;'> in</span> <span style='color:red; font-weight:bold;'>1</span><span style='color:red; font-weight:bold;'>9</span><span style='color:red; font-weight:bold;'>9</span><span style='color:red; font-weight:bold;'>3</span>.</s>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "highlight_uncertain_claims(\n",
    "    output[\"uncertainty_score\"][0],\n",
    "    output[\"sequences\"][:,len(inputs[\"input_ids\"][0]):][0],\n",
    "    [claim.aligned_token_ids for claim in output.deps[\"claims\"][0]]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
