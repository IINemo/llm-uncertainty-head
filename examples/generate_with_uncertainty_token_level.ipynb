{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "uhead_name = \"llm-uncertainty-head/uhead_Mistral-7B-Instruct-v0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from luh import AutoUncertaintyHead, CausalLMWithUncertainty\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "uhead = AutoUncertaintyHead.from_pretrained(\n",
    "    uhead_name, base_model=llm)\n",
    "llm_adapter = CausalLMWithUncertainty(llm, uhead, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text and prepare inputs ...\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"How many fingers are on a coala's foot?\"\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Who sang a song Yesterday?\"\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Who sang a song Кукла Колдуна?\"\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Translate into French: 'I want a small cup of coffee'\"\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "\n",
    "chat_messages = [tokenizer.apply_chat_template(m, tokenize=False, add_bos_token=False) for m in messages]\n",
    "inputs = tokenizer(chat_messages, return_tensors=\"pt\", padding=True, truncation=True, add_special_tokens=False).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm_adapter.generate(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_html_tokens(\n",
    "    token_ids,\n",
    "    positions_to_highlight,\n",
    "    tokenizer,\n",
    "    color=\"red\",\n",
    "    font_weight=\"bold\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert a list of token IDs into a readable string, highlight tokens at\n",
    "    the specified positions in `positions_to_highlight`, and remove the leading\n",
    "    '▁' that Mistral/Llama tokenizers use for word boundaries.\n",
    "    \n",
    "    Args:\n",
    "        token_ids (List[int]): The sequence of token IDs.\n",
    "        tokenizer: A Hugging Face tokenizer (e.g., for mistralai/Mistral-7B-Instruct-v0.2).\n",
    "        positions_to_highlight (Set[int] or List[int]): 0-based indices of tokens to highlight.\n",
    "        color (str): CSS color for the highlighted text (default \"red\").\n",
    "        font_weight (str): CSS font weight (default \"bold\").\n",
    "    \n",
    "    Returns:\n",
    "        str: An HTML string with some tokens highlighted.\n",
    "    \"\"\"\n",
    "    # Convert the IDs to subword tokens (may contain leading \"▁\")\n",
    "    raw_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    \n",
    "    # Ensure positions_to_highlight is a set for quick membership check\n",
    "    if not isinstance(positions_to_highlight, set):\n",
    "        positions_to_highlight = set(positions_to_highlight)\n",
    "    \n",
    "    final_pieces = []\n",
    "    \n",
    "    for idx, token in enumerate(raw_tokens):\n",
    "        # If the token starts with \"▁\", replace that with a literal space\n",
    "        if token.startswith(\"▁\"):\n",
    "            display_str = \" \" + token[1:]\n",
    "        else:\n",
    "            display_str = token\n",
    "        \n",
    "        # If this position is in positions_to_highlight, wrap in <span>\n",
    "        if idx in positions_to_highlight:\n",
    "            display_str = (\n",
    "                f\"<span style='color:{color}; font-weight:{font_weight};'>\"\n",
    "                f\"{display_str}\"\n",
    "                \"</span>\"\n",
    "            )\n",
    "        \n",
    "        final_pieces.append(display_str)\n",
    "    \n",
    "    # Join everything without extra spaces\n",
    "    return \"\".join(final_pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "def highlight_uncertain_tokens(generated_tokens, uncertain_logits, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Highlight tokens in a generated sequence based on their uncertainty scores.\n",
    "    \n",
    "    Args:\n",
    "        generated_tokens (List[int]): The sequence of generated token IDs.\n",
    "        uncertain_logits (List[float]): The uncertainty scores for each token.\n",
    "        threshold (float): The threshold for considering a token uncertain (default 0.5).\n",
    "    \n",
    "    Returns:\n",
    "        str: An HTML string with some tokens highlighted based on uncertainty.\n",
    "    \"\"\"\n",
    "    # Find the positions of uncertain tokens\n",
    "    uncertain_probs = expit(uncertain_logits)\n",
    "    uncertain_positions = [idx for idx, score in enumerate(uncertain_probs) if score > threshold]\n",
    "    \n",
    "    # Highlight the tokens at those positions\n",
    "    return display(HTML(highlight_html_tokens(generated_tokens, uncertain_positions, tokenizer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2\n",
    "highlight_uncertain_tokens(output[\"greedy_tokens\"][idx], output[\"uncertainty_logits\"][idx], threshold=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
