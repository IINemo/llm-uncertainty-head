{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "576cbbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/gkuzmin/llm_ue/lm_uhead_paper_code_prepare/conda_uhead_paper/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from luh import AutoUncertaintyHead\n",
    "\n",
    "from lm_polygraph import CausalLMWithUncertainty\n",
    "\n",
    "from luh.calculator_infer_luh import CalculatorInferLuh\n",
    "from luh.calculator_apply_uq_head import CalculatorApplyUQHead\n",
    "from luh.luh_estimator_dummy import LuhEstimatorDummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e00cc0fc-cd7c-49dc-a38b-52880686091d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:28<00:00,  9.34s/it]\n"
     ]
    }
   ],
   "source": [
    "# load model and uhead\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "uhead_name = \"llm-uncertainty-head/uhead_Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "uhead = AutoUncertaintyHead.from_pretrained(\n",
    "    uhead_name, base_model=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63f267be-faf2-4615-906e-223e13287950",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "args_generate = {\"generation_config\": generation_config,\n",
    "                 \"max_new_tokens\": 50}\n",
    "calc_infer_llm = CalculatorInferLuh(uhead, \n",
    "                                    tokenize=True, \n",
    "                                    args_generate=args_generate,\n",
    "                                    device=\"cuda\",\n",
    "                                    generations_cache_dir=\"\",\n",
    "                                    predict_token_uncertainties=True)\n",
    "\n",
    "estimator = LuhEstimatorDummy()\n",
    "llm_adapter = CausalLMWithUncertainty(llm, tokenizer=tokenizer, stat_calculators=[calc_infer_llm], estimator=estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a179b2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['sequences', 'scores', 'attentions', 'hidden_states', 'past_key_values', 'full_attention_mask', 'context_lengths'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/gkuzmin/llm_ue/lm_uhead_paper_code_prepare/conda_uhead_paper/lib/python3.11/site-packages/torch/nn/modules/transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.6172351837158203,\n",
       "  0.4881523847579956,\n",
       "  0.5772963762283325,\n",
       "  0.7417271733283997,\n",
       "  0.7467945218086243,\n",
       "  0.393752783536911,\n",
       "  0.4387787878513336,\n",
       "  0.5772822499275208,\n",
       "  0.6776686906814575,\n",
       "  0.7247191071510315,\n",
       "  0.8887840509414673,\n",
       "  0.9094085097312927,\n",
       "  0.8421838283538818,\n",
       "  0.8859848380088806,\n",
       "  0.851408839225769,\n",
       "  0.8874452114105225,\n",
       "  0.8746453523635864,\n",
       "  0.9152727127075195,\n",
       "  0.8852745890617371,\n",
       "  0.8997197151184082]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# prepare text ...\n",
    "\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"In which year did the programming language Mercury first appear? Answer with a year only.\"\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "# The correct answer is 1995\n",
    "\n",
    "chat_messages = [tokenizer.apply_chat_template(m, tokenize=False, add_bos_token=False) for m in messages]\n",
    "inputs = tokenizer(chat_messages, return_tensors=\"pt\", padding=True, truncation=True, add_special_tokens=False).to(\"cuda\")\n",
    "\n",
    "output = llm_adapter.generate(inputs[\"input_ids\"])\n",
    "output[\"uncertainty_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2dc2b4c-23df-49ee-8e58-ab65f2549bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response and uncertainty scores:\n",
      "Response: ['Mercury is a logic programming language that was first announced in 1993. However,']\n",
      "UE Scores: [0.6172351837158203, 0.4881523847579956, 0.5772963762283325, 0.7417271733283997, 0.7467945218086243, 0.393752783536911, 0.4387787878513336, 0.5772822499275208, 0.6776686906814575, 0.7247191071510315, 0.8887840509414673, 0.9094085097312927, 0.8421838283538818, 0.8859848380088806, 0.851408839225769, 0.8874452114105225, 0.8746453523635864, 0.9152727127075195, 0.8852745890617371, 0.8997197151184082]\n"
     ]
    }
   ],
   "source": [
    "print(\"Model response and uncertainty scores:\")\n",
    "print(f'Response: {tokenizer.batch_decode(output[\"sequences\"][:,len(inputs[\"input_ids\"][0]):])}')\n",
    "print(f'UE Scores: {output[\"uncertainty_score\"][0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ebd3692-c8c7-42d4-8c23-76e760994176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_html_tokens(\n",
    "    token_ids,\n",
    "    positions_to_highlight,\n",
    "    tokenizer,\n",
    "    color=\"red\",\n",
    "    font_weight=\"bold\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert a list of token IDs into a readable string, highlight tokens at\n",
    "    the specified positions in `positions_to_highlight`, and remove the leading\n",
    "    '▁' that Mistral/Llama tokenizers use for word boundaries.\n",
    "    \n",
    "    Args:\n",
    "        token_ids (List[int]): The sequence of token IDs.\n",
    "        tokenizer: A Hugging Face tokenizer (e.g., for mistralai/Mistral-7B-Instruct-v0.2).\n",
    "        positions_to_highlight (Set[int] or List[int]): 0-based indices of tokens to highlight.\n",
    "        color (str): CSS color for the highlighted text (default \"red\").\n",
    "        font_weight (str): CSS font weight (default \"bold\").\n",
    "    \n",
    "    Returns:\n",
    "        str: An HTML string with some tokens highlighted.\n",
    "    \"\"\"\n",
    "    # Convert the IDs to subword tokens (may contain leading \"▁\")\n",
    "    raw_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    \n",
    "    # Ensure positions_to_highlight is a set for quick membership check\n",
    "    if not isinstance(positions_to_highlight, set):\n",
    "        positions_to_highlight = set(positions_to_highlight)\n",
    "    \n",
    "    final_pieces = []\n",
    "    \n",
    "    for idx, token in enumerate(raw_tokens):\n",
    "        # If the token starts with \"▁\", replace that with a literal space\n",
    "        if token.startswith(\"▁\"):\n",
    "            display_str = \" \" + token[1:]\n",
    "        else:\n",
    "            display_str = token\n",
    "        \n",
    "        # If this position is in positions_to_highlight, wrap in <span>\n",
    "        if idx in positions_to_highlight:\n",
    "            display_str = (\n",
    "                f\"<span style='color:{color}; font-weight:{font_weight};'>\"\n",
    "                f\"{display_str}\"\n",
    "                \"</span>\"\n",
    "            )\n",
    "        \n",
    "        final_pieces.append(display_str)\n",
    "    \n",
    "    # Join everything without extra spaces\n",
    "    return \"\".join(final_pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef15b565-9e2d-434e-b2dc-983a654f016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "def highlight_uncertain_claims(uncertainties, generated_tokens, claims):\n",
    "    threshold = 0.5\n",
    "    tokens_to_highlight = set()\n",
    "\n",
    "    for ue_score, claim in zip(uncertainties, claims):\n",
    "        if ue_score > threshold:\n",
    "            tokens_to_highlight.update([claim])\n",
    "    \n",
    "    display(HTML(highlight_html_tokens(generated_tokens, tokens_to_highlight, tokenizer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f1a2774-fae3-4474-893a-1cddaae11cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style='color:red; font-weight:bold;'> Mer</span>cury<span style='color:red; font-weight:bold;'> is</span><span style='color:red; font-weight:bold;'> a</span><span style='color:red; font-weight:bold;'> logic</span> programming language<span style='color:red; font-weight:bold;'> that</span><span style='color:red; font-weight:bold;'> was</span><span style='color:red; font-weight:bold;'> first</span><span style='color:red; font-weight:bold;'> announced</span><span style='color:red; font-weight:bold;'> in</span><span style='color:red; font-weight:bold;'> </span><span style='color:red; font-weight:bold;'>1</span><span style='color:red; font-weight:bold;'>9</span><span style='color:red; font-weight:bold;'>9</span><span style='color:red; font-weight:bold;'>3</span><span style='color:red; font-weight:bold;'>.</span><span style='color:red; font-weight:bold;'> However</span><span style='color:red; font-weight:bold;'>,</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "highlight_uncertain_claims(\n",
    "    output[\"uncertainty_score\"][0],\n",
    "    output[\"sequences\"][:,len(inputs[\"input_ids\"][0]):][0],\n",
    "    list(range(len(output[\"sequences\"][:,len(inputs[\"input_ids\"][0]):][0]))),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deda943c-4ee6-4696-8381-ed0b601b7e86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eba3748-7fe0-41f4-85aa-5b320f1c23b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1f38ac-f2e1-43a6-8d1f-5c4bea0c4259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda_uhead_paper-2]",
   "language": "python",
   "name": "conda-env-conda_uhead_paper-2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
