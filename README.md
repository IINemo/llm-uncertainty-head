![image](https://github.com/user-attachments/assets/51ee8bd1-79f6-4f00-abe6-e8c875df96c2)[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://github.com/IINemo/llm-uncertainty-head/blob/master/LICENSE)
![Python 3.11](https://img.shields.io/badge/python-3.11-blue.svg)
[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Benchmark-yellow)](https://huggingface.co/llm-uncertainty-head)

# LLM Uncertainty Head

[Installation](#installation) | [Basic usage](#basic_usage) 

Pre-trained UQ heads -- supervised auxiliary modules for LLMs that substantially enhance their ability to capture uncertainty. A powerful Transformer architecture in their design and informative features derived from LLM attention maps enable strong performance, as well as cross-lingual and cross-domain generalization.

## Installation

## Basic usage
