{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd20a517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a built-in dataset, e.g., 'imdb'\n",
    "dataset = load_dataset(\"llm-uncertainty-head/train_akimbio_mistral\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6b7099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Replace with the exact model you're using, e.g., 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "# Example input_ids (truncated from your example)\n",
    "input_ids = [1, 733, 16289, 28793, 15259, 528, 264, 17004, 302, 14003]\n",
    "\n",
    "# Detokenize\n",
    "decoded_text = tokenizer.decode(dataset['train'][0]['input_ids'], skip_special_tokens=True)\n",
    "\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791b0744",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"llm-uncertainty-head/train_akimbio_mistral\")\n",
    "train_data = dataset[\"train\"]\n",
    "dev_data = dataset[\"eval\"]\n",
    "\n",
    "def decode_input_ids(input_ids):\n",
    "    return tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "\n",
    "def format_claims(claims):\n",
    "    formatted = []\n",
    "    for c in claims:\n",
    "        if \"claim_text\" in c and \"aligned_token_ids\" in c:\n",
    "            claim = c[\"claim_text\"].strip().replace(\"\\n\", \" \")\n",
    "            ids = \",\".join(str(i) for i in c[\"aligned_token_ids\"])\n",
    "            formatted.append(f\"{claim} ||| {ids}\")\n",
    "    return \"\\n\".join(formatted)\n",
    "\n",
    "def process_dataset(split_data):\n",
    "    results = []\n",
    "    for example in tqdm(split_data, desc=\"Processing\"):\n",
    "        input_text = decode_input_ids(example[\"input_ids\"])\n",
    "        output_text = format_claims(example[\"claims\"])\n",
    "        results.append({\n",
    "            \"input_text\": input_text,\n",
    "            \"target_text\": output_text\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Process and save\n",
    "train_processed = process_dataset(train_data)\n",
    "dev_processed = process_dataset(dev_data)\n",
    "\n",
    "# Optional: Save to disk\n",
    "with open(\"train_claims.jsonl\", \"w\") as f:\n",
    "    for ex in train_processed:\n",
    "        f.write(json.dumps(ex, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "with open(\"dev_claims.jsonl\", \"w\") as f:\n",
    "    for ex in dev_processed:\n",
    "        f.write(json.dumps(ex, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"✅ Done. Files saved: train_claims.jsonl, dev_claims.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ac1ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Restrict to GPU 0 only\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# --- Configuration ---\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "dataset_path = {\n",
    "    \"train\": \"train_claims.jsonl\",\n",
    "    \"validation\": \"dev_claims.jsonl\"\n",
    "}\n",
    "output_dir = \"./qwen3-06b-qlora-claims\"\n",
    "\n",
    "# --- Load dataset ---\n",
    "dataset = load_dataset(\"json\", data_files=dataset_path)\n",
    "\n",
    "# --- Load tokenizer ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# --- Preprocessing ---\n",
    "def preprocess(example):\n",
    "    input_enc = tokenizer(\n",
    "        example[\"input_text\"],\n",
    "        max_length=1024,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    target_enc = tokenizer(\n",
    "        example[\"target_text\"],\n",
    "        max_length=1024,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Ensure labels match input length and are masked correctly\n",
    "    labels = target_enc[\"input_ids\"]\n",
    "    labels = [label if label != tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_enc[\"input_ids\"],\n",
    "        \"attention_mask\": input_enc[\"attention_mask\"],\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "# --- Force single GPU usage ---\n",
    "torch.cuda.set_device(0)\n",
    "device_map = {\"\": 0}  # All tensors on GPU 0\n",
    "\n",
    "# --- Load model with QLoRA config ---\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# --- Training setup ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=20,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# --- Train ---\n",
    "trainer.train()\n",
    "\n",
    "# --- Save final adapter ---\n",
    "model.save_pretrained(output_dir + \"/final\")\n",
    "tokenizer.save_pretrained(output_dir + \"/final\")\n",
    "print(\"✅ Training complete. Model saved to:\", output_dir + \"/final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510657fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Load model and tokenizer ---\n",
    "model_dir = \"./qwen3-06b-qlora-claims/final\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True)\n",
    "\n",
    "# --- Load validation set ---\n",
    "dataset = load_dataset(\"json\", data_files={\"validation\": \"dev_claims.jsonl\"})[\"validation\"]\n",
    "\n",
    "# --- Generate predictions ---\n",
    "def generate_claims(example, max_new_tokens=256):\n",
    "    input_ids = tokenizer(example[\"input_text\"], return_tensors=\"pt\", truncation=True, max_length=512).input_ids.to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return decoded\n",
    "\n",
    "# --- Run on all dev examples ---\n",
    "predictions = []\n",
    "for example in tqdm(dataset):\n",
    "    pred = generate_claims(example)\n",
    "    predictions.append({\n",
    "        \"input_text\": example[\"input_text\"],\n",
    "        \"target_text\": example[\"target_text\"],\n",
    "        \"predicted_text\": pred\n",
    "    })\n",
    "\n",
    "# --- Save results (optional) ---\n",
    "import json\n",
    "with open(\"dev_predictions.jsonl\", \"w\") as f:\n",
    "    for item in predictions:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"✅ Inference complete. Saved to dev_predictions.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1885b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load your fine-tuned model\n",
    "model_dir = \"./qwen3-06b-qlora-claims/final\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True)\n",
    "model.eval()\n",
    "\n",
    "# Your input (from test or dev)\n",
    "input_text = \"[INST] Tell me a bio of Alice Walker. [/INST] Alice Walker (born February 9, 1944) is an American novelist, short story writer, poet, and activist. She is best known for her novel \\\"The Color Purple\\\" (1982), which gained critical and commercial success and was adapted into a successful film in 1985. The novel explores the experiences of African American women in the rural South and has become a classic of contemporary literature.\\n\\nWalker was born in Eatonton, Georgia, and grew up in a family of sharecroppers. She attended Spelman College in Atlanta, where she studied art and writing. After graduating, she worked as a social worker and a teacher before moving to Mississippi to join the Civil Rights Movement. She became involved with the Student Nonviolent Coordinating Committee (SNCC) and the Black Panther Party.\\n\\nWalker's writing career began in the late 1960s, and she published her first collection of poems, \\\"Once,\\\" in 1966. Her first novel, \\\"The Third Life of Grange Copeland,\\\" was published in 197\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=False,           # greedy decoding for determinism\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "# Decode output\n",
    "decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Remove input_text if it's repeated in output (common in causal LMs)\n",
    "if decoded.startswith(input_text):\n",
    "    decoded = decoded[len(input_text):].strip()\n",
    "\n",
    "# Print result\n",
    "print(\"📥 Input:\")\n",
    "print(input_text)\n",
    "print(\"\\n📤 Generated Claims:\")\n",
    "print(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7abe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Load test set\n",
    "dataset = load_dataset(\"llm-uncertainty-head/train_akimbio_mistral\")\n",
    "test_set = dataset[\"test\"]\n",
    "\n",
    "# Load fine-tuned model\n",
    "model_dir = \"./qwen3-06b-qlora-claims/final\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True)\n",
    "model.eval()\n",
    "\n",
    "# Inference function\n",
    "def generate_claims(question, reply):\n",
    "    model_input = f\"[INST] {question} [/INST] {reply}\"\n",
    "    inputs = tokenizer(model_input, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    if decoded.startswith(model_input):\n",
    "        decoded = decoded[len(model_input):].strip()\n",
    "    return decoded\n",
    "\n",
    "# Generate and save predictions\n",
    "predictions = []\n",
    "for example in tqdm(test_set):\n",
    "    question = example[\"question\"]\n",
    "    reply = example[\"reply\"]\n",
    "    pred = generate_claims(question, reply)\n",
    "    predictions.append({\n",
    "        \"question\": question,\n",
    "        \"reply\": reply,\n",
    "        \"predicted_claims\": pred\n",
    "    })\n",
    "\n",
    "# Save to JSONL\n",
    "with open(\"qwen3_test_predictions.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for p in predictions:\n",
    "        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"✅ Inference complete. Saved to qwen3_test_predictions.jsonl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
