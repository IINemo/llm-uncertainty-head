import hydra
from pathlib import Path
import os
from tqdm import tqdm
import numpy as np

from transformers import AutoTokenizer
from datasets import Dataset, DatasetDict

from lm_polygraph.stat_calculators import ClaimsExtractor
from lm_polygraph.utils.openai_chat import OpenAIChat
from lm_polygraph.generation_metrics.openai_fact_check import OpenAIFactCheck
from lm_polygraph.utils.model import WhiteboxModel
from lm_polygraph.stat_calculators.claim_level_prompts import CLAIM_EXTRACTION_PROMPTS, MATCHING_PROMPTS, OPENAI_FACT_CHECK_PROMPTS, OPENAI_FACT_CHECK_SUMMARIZE_PROMPT

from luh.utils import load_any_dataset

import logging

log = logging.getLogger()

my_logger = logging.getLogger("httpx")
my_logger.setLevel(logging.CRITICAL)


def generate_targets(dataset, reply_tokens_all, tokenizer):
    targets = []

    all_claims = dataset["claims"]
    all_verified = dataset["verified"]
    all_questions = dataset["question"]
    all_input_ids = dataset["input_ids"]

    all_tokenized_questions = tokenizer.apply_chat_template(
            [[{"role": "user", "content": all_questions[idx]}] for idx in range(len(dataset))],
            tokenize=True,
            add_generation_prompt=True,
            return_tensors=None,
            padding=False
        )

    progress_bar = tqdm(total=len(dataset))
    for idx in range(len(dataset)):
        #reply = dataset["reply"][idx]
        reply_tokens = reply_tokens_all[idx]
        #reply_tokens = tokenizer(reply, add_special_tokens=False)["input_ids"]
        claims = all_claims[idx]
        verified = all_verified[idx]
        input_ids = all_input_ids[idx]

        target = [-100.] * len(reply_tokens)
        for claim, label in zip(claims, verified):
            for t in claim["aligned_token_ids"]:
                if not np.isnan(label):
                    target[t] = float(label == 1.0)

                # if target[t] != 0. and not np.isnan(label): # We do not mark as hallucinations tokens that belong to hallucinated and correct claims
                #     target[t] = float(label == 1.0)
                
                # if not np.isnan(label):
                #     if label == 1.0:
                #         # Always mark token as hallucinated (1)
                #         target[t] = 1.0
                #     elif label == 0.0:
                #         # Mark token as correct (0) only if it hasn't already been flagged as hallucinated
                #         if target[t] != 1.0:
                #             target[t] = 0.0

        # encoded_question = tokenizer.apply_chat_template(
        #     [{"role": "user", "content": all_questions[idx]}],
        #     tokenize=True,
        #     add_generation_prompt=True,
        #     return_tensors="pt",
        # )
        encoded_question = all_tokenized_questions[idx]
        final_target = [-100] * len(encoded_question) # TODO: check this is the right dim
        curr_target = 0
        for i in range(len(encoded_question), len(input_ids)):
            if input_ids[i] in tokenizer.all_special_ids:
                final_target.append(-100)
            else:
                final_target.append(target[curr_target])
                curr_target += 1

        #final_target = final_target + [-100] * (len(reply_tokens) - len(final_target))

        assert len(final_target) == len(input_ids)
        targets.append(
            final_target
        ) 
        progress_bar.update(1)
    
    progress_bar.close()

    return targets


def extract_tokens_of_reply(dataset, tokenizer):
    inpt_texts = dataset["question"]
    question_tokens = tokenizer.apply_chat_template(
            [[{"role": "user", "content": e}] for e in inpt_texts],
            tokenize=True,
            add_generation_prompt=True,
        )
    greedy_tokens = []
    inpt_ids = dataset["input_ids"]
    for i in range(len(dataset)): # This whole thing is necessary because we need tokens generated by LLM not how reply will be tokenized
        tokens_with_special = inpt_ids[i][len(question_tokens[i]):]
        greedy_tokens.append(
            [t for t in tokens_with_special if t not in tokenizer.all_special_ids]
        )
    
    return greedy_tokens


def get_prompts():
    result = {
        "extraction": CLAIM_EXTRACTION_PROMPTS,
        "matching": MATCHING_PROMPTS,
        "fact_check": OPENAI_FACT_CHECK_PROMPTS,
        "summarize": OPENAI_FACT_CHECK_SUMMARIZE_PROMPT,
    }

    import importlib
    package_name = "benchmark_multilingual"
    bench = importlib.import_module(package_name)
    package_dir = os.path.dirname(bench.__file__)

    for lang in os.listdir(package_dir):
        
        if not os.path.isdir(Path(package_dir) / lang):
            continue
        
        try:
            module_name = f"{package_name}.{lang}.prompts"
            mod = importlib.import_module(module_name)

            log.info(f"Loading custom prompts for language: {lang}")

            result["extraction"][module_name].update(mod.CLAIM_EXTRACTION_PROMPTS)
            result["matching"][module_name].update(mod.MATCHING_PROMPTS)
            result["fact_check"][module_name].update(mod.OPENAI_FACT_CHECK_PROMPTS)
            result["summarize"][module_name].update(mod.FACT_CHECK_SUMMARIZE_PROMPT)
        except:
            continue

    return result

    # if lang not in CLAIM_EXTRACTION_PROMPTS:
    #     import importlib
    #     lang_mod = importlib.import_module(f"bencmhark_multilingual.{lang}")

    #     return {
    #         "extraction": lang_mod.CLAIM_EXTRACTION_PROMPTS,
    #         "matching": lang_mod.MATCHING_PROMPTS,
    #         "fact_check": lang_mod.OPENAI_FACT_CHECK_PROMPTS,
    #         "summarize": lang_mod.FACT_CHECK_SUMMARIZE_PROMPT,
    #     }
    
    # else:
    #     return {
    #         "extraction": CLAIM_EXTRACTION_PROMPTS,
    #         "matching": MATCHING_PROMPTS,
    #         "fact_check": OPENAI_FACT_CHECK_PROMPTS,
    #         "summarize": OPENAI_FACT_CHECK_SUMMARIZE_PROMPT,
    #     }


@hydra.main(
    version_base=None,
    config_path=str(Path(os.environ.get("HYDRA_CONFIG", "")).parent),
    config_name=str(Path(os.environ.get("HYDRA_CONFIG", "")).name),
)
def main(config):
    output_dir = hydra.core.hydra_config.HydraConfig.get().runtime.output_dir

    tokenizer = AutoTokenizer.from_pretrained(config.tokenizer)

    log.info("Loading dataset...")
    dataset_dict = load_any_dataset(config.dataset.path, config)
    log.info("Done.")

    prompts = get_prompts()
    chat_gpt_claim_extractor = OpenAIChat(
        openai_model=config.claim_extractor.openai_model, cache_path=config.cache_path
    )
    claim_extractor = ClaimsExtractor(
        chat_gpt_claim_extractor, 
        language=config.language, 
        progress_bar=True,
        extraction_prompts=prompts["extraction"],
        matching_prompts=prompts["matching"],
        n_threads=config.claim_extractor.n_threads
    )

    fact_checker = OpenAIFactCheck(
        llm_url=config.claim_annotator.llm_url,
        openai_model=config.claim_annotator.openai_model,
        language=config.language,
        cache_path=config.cache_path,
        progress_bar=True,
        fact_check_prompts=prompts["fact_check"],
        fact_check_summarize_prompt=prompts["summarize"],
        n_threads=config.claim_annotator.n_threads,
        timeout=config.claim_annotator.timeout,
        max_tokens=config.claim_annotator.max_tokens if hasattr(config.claim_annotator, "max_tokens") else None,
        rewrite_cache=config.claim_annotator.rewrite_cache if hasattr(config.claim_annotator, "rewrite_cache") else False
    )
    model = WhiteboxModel(model=None, tokenizer=tokenizer)

    all_results = dict()
    for split in dataset_dict:
        log.info(f"Working on split {split}...")
        dataset = dataset_dict[split]
        if config.dataset.num_instances > 0:
            dataset = dataset.select(range(min(config.dataset.num_instances, len(dataset))))
        log.info(f"Dataset length: {len(dataset)}")

        log.info("Extracting claims...")
        greedy_tokens = extract_tokens_of_reply(dataset, tokenizer)
        deps = {"greedy_texts": dataset["reply"], "greedy_tokens": greedy_tokens}
        if "claims" in dataset.features and not config.drop_annotations:
            claims = dataset["claims"]
        else:
            claims = claim_extractor(deps, dataset["question"], model=model)
            claims = [
                [e for e in sent_claims]
                for sent_claims in claims["claims"]
            ]
        log.info("Done.")

        log.info("Verifying claims...")
        if "verified" in dataset.features and not config.drop_annotations:
            verified = dataset["verified"]
        else:
            stats = {"input_texts": dataset["question"], "claims": claims}
            verified = fact_checker(stats, None)
        log.info("Done")


        # TODO: fix target generation
        log.info("Generating targets...")
        result = dataset.to_dict()
        result.update(
            {
                "claims": [[claim.__dict__ for claim in e] for e in claims],
                "verified": verified,
            }
        )
        new_dataset = Dataset.from_dict(result)
        result["uncertainty_labels"] = generate_targets(new_dataset, greedy_tokens, tokenizer)
        log.info("Done.")

        all_results[split] = Dataset.from_dict(result)
        log.info(f"Done with split {split}")

    output_path = Path(output_dir) / "result"
    log.info(f"Saving data to: {output_path}")
    dataset_collection = DatasetDict(all_results)
    dataset_collection.save_to_disk(output_path)
    log.info("Done.")


if __name__ == "__main__":
    main()
